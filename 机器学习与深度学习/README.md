# 关于机器学习比较好的文章

## 数学推导
- [凸优化](https://blog.csdn.net/wangchy29/article/details/86499918)
## 机器学习
- [Adaboost算法原理分析和实例+代码（简明易懂）](https://blog.csdn.net/guyuealian/article/details/70995333)
- [Adaboost 算法的原理与推导](https://blog.csdn.net/v_JULY_v/article/details/40718799)
- [集成学习原理小结](https://www.cnblogs.com/pinard/p/6131423.html)
- [[Machine Learning & Algorithm] 随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)
- [深入浅出理解决策树算法（二）-ID3算法与C4.5算法](https://zhuanlan.zhihu.com/p/26760551)
- [GBDT 算法：原理篇](https://cloud.tencent.com/developer/article/1005611)
- [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)
- [LightGBM vs XGBoost](https://zhuanlan.zhihu.com/p/31148458)
- [比XGBOOST更快--LightGBM介绍](https://zhuanlan.zhihu.com/p/25308051)
- [RF、GBDT、XGBoost、lightGBM原理与区别](https://blog.csdn.net/data_scientist/article/details/79022025)
- [XGBoost、LightGBM的详细对比介绍](https://www.cnblogs.com/infaraway/p/7890558.html)
- [XGBoost, LightGBM性能大对比](https://zhuanlan.zhihu.com/p/24498293)
- [xgboost中的数学原理](https://blog.csdn.net/a358463121/article/details/68617389)
- [L1 VS L2 vs ElasticNet 正则化总结](https://www.cnblogs.com/fredkeke/p/9981388.html)
- [逻辑回归（Logistic Regression）](https://zhuanlan.zhihu.com/p/28408516)
- [XGBoost 与 GBDT 的区别](https://blog.csdn.net/jackmcgradylee/article/details/77778001)
- [线性回归 梯度求解](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression)
- [HMM 隐马尔科夫状态序列](https://www.zhihu.com/question/20962240)
## 深度学习
- [理解 LSTM 网络](https://www.jianshu.com/p/9dc9f41f0b29)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## 自然语言处理



## 算法优化
- [克服过拟合和提高泛化能力的20条技巧和诀窍](https://blog.csdn.net/starzhou/article/details/52754436)
- [[精进魔法] Regularization：减少Overfitting ，提高模型泛化能力](https://ithelp.ithome.com.tw/articles/10203371)
## 学习资料
- [BAT机器学习面试1000题系列](https://blog.csdn.net/v_JULY_v/column/info/17609)
